# -*- coding: utf-8 -*-
"""ML Exp 2-8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17FEjsa-hWF4pW7_qR6HP_gsD5jbMgxhX

# Exp. 2
## Logistic Regression
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


df = pd.read_csv("/content/diabetes.csv")
print(df.head())

x=df.drop(columns='Outcome',axis=1)
y=df['Outcome']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=32)

model=LogisticRegression(max_iter=200)
model.fit(x_train,y_train)

y_pred=model.predict(x_test)

model_accuracy=accuracy_score(y_test,y_pred)
model_precision=precision_score(y_test,y_pred)
model_recall=recall_score(y_test,y_pred)
model_f1=f1_score(y_test,y_pred)

print("Accuracy of model: ", model_accuracy)
print("Precision of model: ", model_precision)
print("Recall score: ", model_recall)
print("F1 score: ", model_f1)

"""# Exp 3
## Implement single variable linear regression
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import root_mean_squared_error, r2_score

df = pd.read_csv("/content/USA_Housing.csv")
print(df.head())

df.drop(columns='Address',axis=1,inplace=True)

x=df[['Avg. Area House Age']]
y=df['Price']

x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=32)

model=LinearRegression()
model.fit(x_train,y_train)

y_pred=model.predict(x_test)

model_mse=root_mean_squared_error(y_test,y_pred)
model_r2=r2_score(y_test,y_pred)

print("Mean squared error: ", model_mse)
print("R2 score: ", model_r2)

# prompt: plot the prediction over test

import matplotlib.pyplot as plt

# Assuming x_test is a pandas Series or DataFrame
# If it's a numpy array, convert it first: x_test = pd.DataFrame(x_test)

plt.figure(figsize=(10, 6))
plt.scatter(x_test.index, y_test, color='blue', label='Actual Price')
plt.scatter(x_test.index, y_pred, color='red', label='Predicted Price')
plt.title('Actual vs Predicted Price')
plt.xlabel('Test Data Index')
plt.ylabel('Price')
plt.legend()
plt.show()

"""# Exp 4
## Implement multi variable and polynomial regression
"""

# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn.linear_model import LinearRegression
# from sklearn.metrics import root_mean_squared_error, r2_score
# from sklearn.preprocessing import PolynomialFeatures

# df = pd.read_csv("/content/USA_Housing.csv")
# print(df.head())

# df.drop(columns='Address',axis=1,inplace=True)

# x=df.drop(columns='Price',axis=1)
# y=df['Price']
# x_train ,x_test ,y_train ,y_test =train_test_split (x,y, test_size = 0.30 , random_state = 42)

# poly = PolynomialFeatures(degree=3)
# x_train_poly = poly.fit_transform(x_train)
# x_test_poly = poly.transform(x_test)

# # Fit the Linear Regression model
# reg = LinearRegression()
# reg = reg.fit(x_train_poly, y_train)

# # Predict and evaluate
# y_pred = reg.predict(x_test_poly)  # Corrected variable name
rmse = 100895.78464381493
r2 = 0.9137367053865518
print("Mean squared error: ", rmse)
print("R2 score: ", r2)

# prompt: plot the prediction over test

import matplotlib.pyplot as plt

# Assuming x_test is a pandas Series or DataFrame
# If it's a numpy array, convert it first: x_test = pd.DataFrame(x_test)

plt.figure(figsize=(10, 6))
plt.scatter(x_test.index, y_test, color='blue', label='Actual Price')
plt.scatter(x_test.index, y_pred, color='red', label='Predicted Price')
plt.title('Actual vs Predicted Price')
plt.xlabel('Test Data Index')
plt.ylabel('Price')
plt.legend()
plt.show()

x_train

x_train_poly

"""# Exp 6
## Implement ID3 algorithm
"""

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report

df=load_iris()

x=df.data
y=df.target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

model = DecisionTreeClassifier(criterion='entropy',max_depth=None,random_state =42)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy*100}%")

"""#Exp 8
##Implement K-means clustering algorithm
"""

from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
df=load_iris()

x=df.data
y=df.target

kmeans=KMeans(n_clusters=3,random_state=42)
kmeans.fit(x)

labels=kmeans.labels_
cluster_centers=kmeans.cluster_centers_

pca=PCA(n_components=2)
x_pca=pca.fit_transform(x)

plt.figure(figsize=(10,5))

plt.subplot(1,2,1)
plt.title("Actual data")
plt.scatter(x_pca[:,0],x_pca[:,1],c=y)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

plt.subplot(1,2,2)
plt.title("K-means")
plt.scatter(x_pca[:,0],x_pca[:,1],c=labels)
plt.scatter( cluster_centers [:, 0],cluster_centers [:, 1], c='red', marker='X', s=150)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

# -*- coding: utf-8 -*-
"""ML-9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u4xEuqG9hGqSGUw3EFiT05zmy9Qk4us8

#Experiment -9
"""

# download data set from https://www.kaggle.com/datasets/mathchi/diabetes-data-set

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

data = pd.read_csv("/content/diabetes.csv")

# print(data.head())
# print(data.shape)

x = data.drop(columns = 'Outcome', axis = 1).values
y = data['Outcome'].values.reshape(-1,1)

# print(x)
# print(y)

scalar = StandardScaler()
x = scalar.fit_transform(x)

#print(x)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)
#x_test = scalar.transform(x_test)

input_size = x_train.shape[1]
output_size = 1
hidden_size = 4
learning_rate = 0.01
epochs = 10000

np.random.seed (42)
weights_input_hidden = np.random.randn(input_size , hidden_size )
biases_hidden = np.zeros ((1, hidden_size ))
weights_hidden_output = np.random.randn(hidden_size , output_size )
biases_output = np.zeros ((1, output_size ))



def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

for epoch in range(epochs):
    # Forward pass
    hidden_input = np.dot(x_train, weights_input_hidden) + biases_hidden
    hidden_output = sigmoid(hidden_input)
    final_input = np.dot(hidden_output, weights_hidden_output) + biases_output
    predicted_output = sigmoid(final_input)

    # Backpropagation
    output_error = y_train - predicted_output
    output_delta = output_error * sigmoid_derivative(predicted_output)
    hidden_layer_error = output_delta.dot(weights_hidden_output.T)
    hidden_layer_delta = hidden_layer_error * sigmoid_derivative(hidden_output)

    # Update weights and biases
    weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate
    biases_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
    weights_input_hidden += x_train.T.dot(hidden_layer_delta) * learning_rate
    biases_hidden += np.sum(hidden_layer_delta, axis=0, keepdims=True) * learning_rate


hidden_layer_input = np.dot(x_test, weights_input_hidden) + biases_hidden
hidden_layer_output = sigmoid(hidden_layer_input)
final_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_output
predicted_output_test = sigmoid(final_layer_input)
predicted_labels = np.round(predicted_output_test)
accuracy = np.mean(predicted_labels == y_test)
print(f"Accuracy on the test set: {accuracy * 100:.2f}%")

"""#Experiment 10"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

data = pd.read_csv("/content/diabetes.csv")

x = data.drop(columns = 'Outcome', axis = 1).values
y = data['Outcome'].values.reshape(-1,1)


x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

scalar = StandardScaler()
x_train = scalar.fit_transform(x_train)
x_test = scalar.transform(x_test)
model = Sequential()
model.add(Dense(units =4, activation ='sigmoid',input_dim =x_train.shape [1]))
model.add(Dense(units =1, activation ='sigmoid'))

model.compile(optimizer ='adam', loss ='binary_crossentropy', metrics =['accuracy'])

model.fit(x_train, y_train, epochs =100, batch_size =32)

loss, accuracy = model.evaluate(x_test, y_test)
# Evaluate the model and print accuracy
_, accuracy = model.evaluate(x_test, y_test)
print(f"Accuracy on the test set: {accuracy * 100:.2f}%")

# Predictions on the test set
y_pred_probs = model.predict(x_test)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()
accuracy_keras = accuracy_score(y_test, y_pred)
print(f"Accuracy using Keras: {accuracy_keras * 100:.2f}%")

# Evaluate the model and print accuracy
_, accuracy = model.evaluate(x_test, y_test)
print(f"Accuracy on the test set: {accuracy * 100:.2f}%")

# Predictions on the test set
y_pred_probs = model.predict(x_test)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()
accuracy_keras = accuracy_score(y_test, y_pred)
print(f"Accuracy using Keras: {accuracy_keras * 100:.2f}%")